{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random as rd \n",
    "import numpy as np\n",
    "import text_flappy_bird_gym\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearning agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self,env,epsilon,alpha,gamma,height = 15, width = 20):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor\n",
    "        \"\"\"\n",
    "        # Store the parameters provided in agent_init_info.\n",
    "        self.num_actions = 2\n",
    "        self.num_states = height*width\n",
    "        self.epsilon = epsilon\n",
    "        self.step_size = alpha\n",
    "        self.discount = gamma\n",
    "        self.rand_generator = np.random.RandomState(12)\n",
    "        \n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions) # random action selection\n",
    "        else:\n",
    "            action = self.argmax(current_q) # greedy action selection\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Perform an update\n",
    "        self.q[self.prev_state][self.prev_action] += self.step_size*(reward + self.discount*np.max(self.q[state]) - self.q[self.prev_state][ self.prev_action])\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        # Perform the last update in the episode \n",
    "\n",
    "        self.q[self.prev_state][self.prev_action] += self.step_size*(reward - self.q[self.prev_state][self.prev_action])\n",
    "\n",
    "        \n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)\n",
    "    \n",
    "    def training(self, num_episodes,env):\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "            # monitor progress\n",
    "            if i_episode % 1000 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "\n",
    "            obs = env.reset()\n",
    "            obs = obs[0]\n",
    "\n",
    "            action = self.agent_start(obs)\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                obs, reward, done, _, info = env.step(action)\n",
    "                action = self.agent_step(reward,obs)\n",
    "            \n",
    "            self.agent_end(reward)\n",
    "\n",
    "\n",
    "    def policy(self,obs):\n",
    "        return self.argmax(self.q[obs])\n",
    "    \n",
    "    def get_policy(self):\n",
    "        self.p = dict((k,self.argmax(v)) for k, v in self.q.items())\n",
    "\n",
    "    def get_valuefunction(self):\n",
    "        return dict((k,np.max(v)) for k, v in self.q.items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent():\n",
    "    def __init__(self,env,epsilon,alpha,gamma,height = 15, width = 20):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "        \n",
    "        Args:\n",
    "            epsilon (float): The epsilon parameter for exploration,\n",
    "            step_size (float): The step-size,\n",
    "            discount (float): The discount factor\n",
    "        \"\"\"\n",
    "        # Store the parameters provided in agent_init_info.\n",
    "        self.num_actions = 2\n",
    "        self.num_states = height*width\n",
    "        self.epsilon = epsilon\n",
    "        self.step_size = alpha\n",
    "        self.discount = gamma\n",
    "        self.rand_generator = np.random.RandomState(12)\n",
    "        \n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the episode starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (int): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            action (int): the first action the agent takes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions) # random action selection\n",
    "        else:\n",
    "            action = self.argmax(current_q) # greedy action selection\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (int): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step.\n",
    "        Returns:\n",
    "            action (int): the action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Choose action using epsilon greedy.\n",
    "        current_q = self.q[state]\n",
    "        if self.rand_generator.rand() < self.epsilon:\n",
    "            action = self.rand_generator.randint(self.num_actions)\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        \n",
    "        # Perform an update\n",
    "        self.q[self.prev_state][self.prev_action] += self.step_size*(reward + self.discount*self.q[state][action] - self.q[self.prev_state][ self.prev_action])\n",
    "        \n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        # Perform the last update in the episode \n",
    "\n",
    "        self.q[self.prev_state][self.prev_action] += self.step_size*(reward - self.q[self.prev_state][self.prev_action])\n",
    "\n",
    "        \n",
    "    def argmax(self, q_values):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action-values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)\n",
    "    \n",
    "    def training(self, num_episodes,env):\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "            # monitor progress\n",
    "            if i_episode % 1000 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "\n",
    "            obs = env.reset()\n",
    "            obs = obs[0]\n",
    "\n",
    "            action = self.agent_start(obs)\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                obs, reward, done, _, info = env.step(action)\n",
    "                action = self.agent_step(reward,obs)\n",
    "            \n",
    "            self.agent_end(reward)\n",
    "\n",
    "\n",
    "    def policy(self,obs):\n",
    "        return self.argmax(self.q[obs])\n",
    "    \n",
    "    def get_policy(self):\n",
    "        self.p = dict((k,self.argmax(v)) for k, v in self.q.items())\n",
    "\n",
    "    def get_valuefunction(self):\n",
    "        return dict((k,np.max(v)) for k, v in self.q.items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initiate environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initiate agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(env,epsilon = 0.1,alpha=0.1,gamma= 1.0,height = 15, width = 20)\n",
    "agent.training(num_episodes=50000,env=env)\n",
    "\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "obs = obs[0]\n",
    "\n",
    "actions = []\n",
    "observations = []\n",
    "rewards = []\n",
    "scores = [0]\n",
    "# iterate\n",
    "while scores[-1]<10000:\n",
    "\n",
    "    # Select next action\n",
    "    action = agent.policy(obs)#env.action_space.sample() #agent.policy(obs)  # ## for an agent, action = agent.policy(observation)\n",
    "    actions.append(action)\n",
    "\n",
    "    # Appy action and return new observation of the environment\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    scores.append(info['score'])\n",
    "    observations.append(obs)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    # If player is dead break\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"actions\",actions)\n",
    "print(\"obs\",observations)\n",
    "print(\"rewards\",rewards)\n",
    "print(\"scores\",scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
